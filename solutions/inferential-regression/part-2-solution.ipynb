{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: A Failure Mode for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a feature matrix X with two columns and 100 rows. The first column should be an intercept column of all 1.0's, and the second should be randomly sampled from any distribution (a uniform is fine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros(shape=(100, 2))\n",
    "\n",
    "X[:, 0] = 1.0\n",
    "X[:, 1] = np.random.uniform(size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a target vector from a linear data generating process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use a smaller scale so the parameter estimates have\n",
    "# smaller variance.\n",
    "y = 1.0 + 2.0 * X[:, 1] + np.random.normal(scale=0.25, size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Fit a linear regression to (X, y) data. Look at the fit coefficients (i.e. the parameter estimates in statistical language). Are they what you expect them to be? If you had fit the model to 1,000,000 data points, what would change about them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.878</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.876</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   702.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 03 Nov 2020</td> <th>  Prob (F-statistic):</th> <td>1.71e-46</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:31:46</td>     <th>  Log-Likelihood:    </th> <td>  11.833</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>  -19.67</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>  -14.46</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    1.0444</td> <td>    0.043</td> <td>   24.219</td> <td> 0.000</td> <td>    0.959</td> <td>    1.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    1.9393</td> <td>    0.073</td> <td>   26.508</td> <td> 0.000</td> <td>    1.794</td> <td>    2.084</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.136</td> <th>  Durbin-Watson:     </th> <td>   2.240</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.567</td> <th>  Jarque-Bera (JB):  </th> <td>   1.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.248</td> <th>  Prob(JB):          </th> <td>   0.579</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.873</td> <th>  Cond. No.          </th> <td>    4.31</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.878\n",
       "Model:                            OLS   Adj. R-squared:                  0.876\n",
       "Method:                 Least Squares   F-statistic:                     702.7\n",
       "Date:                Tue, 03 Nov 2020   Prob (F-statistic):           1.71e-46\n",
       "Time:                        16:31:46   Log-Likelihood:                 11.833\n",
       "No. Observations:                 100   AIC:                            -19.67\n",
       "Df Residuals:                      98   BIC:                            -14.46\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          1.0444      0.043     24.219      0.000       0.959       1.130\n",
       "x1             1.9393      0.073     26.508      0.000       1.794       2.084\n",
       "==============================================================================\n",
       "Omnibus:                        1.136   Durbin-Watson:                   2.240\n",
       "Prob(Omnibus):                  0.567   Jarque-Bera (JB):                1.094\n",
       "Skew:                           0.248   Prob(JB):                        0.579\n",
       "Kurtosis:                       2.873   Cond. No.                         4.31\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use more data points then we expect the parameter esitmates to more precisely estimate the true values.  This relies on the data points being independently sampled from one another, and the statistical property that guarentees the better approximation is called **consistency**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create a new feature matrix X with three columns and 100 rows. Make the first two columns the same as your previous X, but make the third column a copy of the second column, i.e., X should have the same data in the second and third column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros(shape=(100, 3))\n",
    "\n",
    "X[:, 0] = 1.0\n",
    "X[:, 1] = np.random.uniform(size=100)\n",
    "X[:, 2] = X[:, 1]\n",
    "\n",
    "y = 1.0 + 2.0 * X[:, 1] + np.random.normal(scale=0.25, size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Fit a linear regression to the new (X, y) data (y should be the same as it was in the previous example). What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jackbennetto/anaconda3/lib/python3.7/site-packages/statsmodels/regression/linear_model.py:300: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if method == \"pinv\":\n",
      "/Users/jackbennetto/anaconda3/lib/python3.7/site-packages/statsmodels/regression/linear_model.py:315: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  elif method == \"qr\":\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "method has to be \"pinv\" or \"qr\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9ce6ec0889f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/statsmodels/regression/linear_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, method, cov_type, cov_kwds, use_t, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'method has to be \"pinv\" or \"qr\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df_model\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: method has to be \"pinv\" or \"qr\""
     ]
    }
   ],
   "source": [
    "model = sm.OLS(y, X)\n",
    "results = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Hopefully you got an error, so there's something unfortunate going on here. Think about what you think the correct answer should be, what coefficients should the model return?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It's not possible to say what the correct answer is in this situation.  We created the `y` vector using the equation:\n",
    "\n",
    "> $$ y = 1 + 2 x_1 + \\epsilon $$\n",
    "\n",
    "> But in our current setup with two copies of the same predictor, this is exactly the same equation as:\n",
    "\n",
    "> $$ y = 1 + 2 x_2 + \\epsilon $$\n",
    "\n",
    "> In fact, there is an infinite number of equivalent expressions:\n",
    "\n",
    "> $$ y = 1 + x_1 + x_2 + \\epsilon $$\n",
    "> $$ y = 1 + 1.5 x_1 + 0.5 x_2 + \\epsilon $$\n",
    "\n",
    "> and so forth.  There's not really a way to say that any one of these is *better* than any other ones, they all give the same answer.\n",
    "\n",
    "> In terms of algebra, we are looking for solutions the the following equation:\n",
    "\n",
    "> $$ X^t X \\beta = X^t y $$\n",
    "\n",
    "> But the *rank* of the matrix $X^t X$ is two, since the columns of $X$ are linearly dependent.  This mean that this system of linear equations has an infinite number of solutions.  This is the source of the error, a *singluar* matrix is one without full rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Create a new feature matrix where one column is a multiple of another, and fit a linear regression again, what happened this time? How can you explain it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros(shape=(100, 3))\n",
    "\n",
    "X[:, 0] = 1.0\n",
    "X[:, 1] = np.random.uniform(size=100)\n",
    "X[:, 2] = 2 * X[:, 1]\n",
    "\n",
    "y = 1.0 + 2.0 * X[:, 1] + np.random.normal(scale=0.25, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "method has to be \"pinv\" or \"qr\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9ce6ec0889f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/statsmodels/regression/linear_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, method, cov_type, cov_kwds, use_t, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'method has to be \"pinv\" or \"qr\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df_model\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: method has to be \"pinv\" or \"qr\""
     ]
    }
   ],
   "source": [
    "model = sm.OLS(y, X)\n",
    "results = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We've got more or less the same issue as before. This time the > following equations are all equivalent:\n",
    "\n",
    "> $$ y = 1 + 2 x_1 + \\epsilon $$\n",
    "> $$ y = 1 + x_2 + \\epsilon $$\n",
    "> $$ y = 1 + x_1 + 0.5 x_2 + \\epsilon $$\n",
    "\n",
    "> So we again have an issue with the rank of the matrix $X^t X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create one last feature matrix where one column is a linear combination of two or more other columns. Fit a linear regression using it. What happened this time? Can you explain it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros(shape=(100, 4))\n",
    "\n",
    "X[:, 0] = 1.0\n",
    "X[:, 1] = np.random.uniform(size=100)\n",
    "X[:, 2] = np.random.uniform(size=100)\n",
    "X[:, 3] = 2 * X[:, 1] - 3 * X[:, 2]\n",
    "\n",
    "y = (1.0\n",
    "     + 2.0 * X[:, 1]\n",
    "     - 1.0 * X[:, 2]\n",
    "     + np.random.normal(scale=0.25, size=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This time the model fit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.875</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.872</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   339.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 03 Nov 2020</td> <th>  Prob (F-statistic):</th> <td>1.59e-44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:31:52</td>     <th>  Log-Likelihood:    </th> <td> 0.13183</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   5.736</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    97</td>      <th>  BIC:               </th> <td>   13.55</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.9915</td> <td>    0.064</td> <td>   15.452</td> <td> 0.000</td> <td>    0.864</td> <td>    1.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.9651</td> <td>    0.067</td> <td>   14.508</td> <td> 0.000</td> <td>    0.833</td> <td>    1.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.4837</td> <td>    0.045</td> <td>   10.713</td> <td> 0.000</td> <td>    0.394</td> <td>    0.573</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.4791</td> <td>    0.022</td> <td>   21.296</td> <td> 0.000</td> <td>    0.434</td> <td>    0.524</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 9.277</td> <th>  Durbin-Watson:     </th> <td>   1.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.010</td> <th>  Jarque-Bera (JB):  </th> <td>   9.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.640</td> <th>  Prob(JB):          </th> <td>  0.0101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.752</td> <th>  Cond. No.          </th> <td>1.81e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 6.99e-31. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.875\n",
       "Model:                            OLS   Adj. R-squared:                  0.872\n",
       "Method:                 Least Squares   F-statistic:                     339.5\n",
       "Date:                Tue, 03 Nov 2020   Prob (F-statistic):           1.59e-44\n",
       "Time:                        16:31:52   Log-Likelihood:                0.13183\n",
       "No. Observations:                 100   AIC:                             5.736\n",
       "Df Residuals:                      97   BIC:                             13.55\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.9915      0.064     15.452      0.000       0.864       1.119\n",
       "x1             0.9651      0.067     14.508      0.000       0.833       1.097\n",
       "x2             0.4837      0.045     10.713      0.000       0.394       0.573\n",
       "x3             0.4791      0.022     21.296      0.000       0.434       0.524\n",
       "==============================================================================\n",
       "Omnibus:                        9.277   Durbin-Watson:                   1.703\n",
       "Prob(Omnibus):                  0.010   Jarque-Bera (JB):                9.188\n",
       "Skew:                          -0.640   Prob(JB):                       0.0101\n",
       "Kurtosis:                       3.752   Cond. No.                     1.81e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 6.99e-31. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> But the coefficients are way off. Note the condition number (\"Cond. No.\") is huge, indicating multicollinearity. In such a case the coefficients can't really be trusted.\n",
    "\n",
    "> This is a sign that we have some problems in our regression.  While before, our regression just failed because we had an exact linear dependency, here floating point error saved us from having an exact problem, but the linear dependency in our matrix has led to badly estimated parameters.\n",
    "\n",
    "> Notice that the parameter estimates returned are very bad estimates of the truth.  This is a common situation when our matrix has either an exact linear dependency, or an approximate one.  The results from these regression should not be trusted!  It's indicating to us that we could remove one of our predictors without suffering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_small = X[:, [0, 1, 2]]\n",
    "\n",
    "model = sm.OLS(y, X_small)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.875</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.872</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   339.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 03 Nov 2020</td> <th>  Prob (F-statistic):</th> <td>1.59e-44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:31:54</td>     <th>  Log-Likelihood:    </th> <td> 0.13183</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   5.736</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    97</td>      <th>  BIC:               </th> <td>   13.55</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.9915</td> <td>    0.064</td> <td>   15.452</td> <td> 0.000</td> <td>    0.864</td> <td>    1.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    1.9232</td> <td>    0.079</td> <td>   24.215</td> <td> 0.000</td> <td>    1.766</td> <td>    2.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -0.9536</td> <td>    0.088</td> <td>  -10.834</td> <td> 0.000</td> <td>   -1.128</td> <td>   -0.779</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 9.277</td> <th>  Durbin-Watson:     </th> <td>   1.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.010</td> <th>  Jarque-Bera (JB):  </th> <td>   9.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.640</td> <th>  Prob(JB):          </th> <td>  0.0101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.752</td> <th>  Cond. No.          </th> <td>    5.26</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.875\n",
       "Model:                            OLS   Adj. R-squared:                  0.872\n",
       "Method:                 Least Squares   F-statistic:                     339.5\n",
       "Date:                Tue, 03 Nov 2020   Prob (F-statistic):           1.59e-44\n",
       "Time:                        16:31:54   Log-Likelihood:                0.13183\n",
       "No. Observations:                 100   AIC:                             5.736\n",
       "Df Residuals:                      97   BIC:                             13.55\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.9915      0.064     15.452      0.000       0.864       1.119\n",
       "x1             1.9232      0.079     24.215      0.000       1.766       2.081\n",
       "x2            -0.9536      0.088    -10.834      0.000      -1.128      -0.779\n",
       "==============================================================================\n",
       "Omnibus:                        9.277   Durbin-Watson:                   1.703\n",
       "Prob(Omnibus):                  0.010   Jarque-Bera (JB):                9.188\n",
       "Skew:                          -0.640   Prob(JB):                       0.0101\n",
       "Kurtosis:                       3.752   Cond. No.                         5.26\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> That's much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Hopefully you've seen a few linear regressions fail at this point. Why did they fail? What is the failure mode for linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Linear regressions fail when the columns of the predictor matrix $X$ are linearly independent, which leads to having multiple (an infinite number of) equally good solutions.  This was explicit in our first few examples.\n",
    "\n",
    "> In our final example, we again had a linear dependency in $X$, but the computer did not catch it exactly.  Instead, it was indicated the large condition number.  This tends to happen when our $X$ matrix either contains an exact or approximate linear independency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
